{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Deep Learning Binary Classification Model with Dropout Regularization"
      ],
      "metadata": {
        "id": "X-DXDxT6VZfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start by importing the necessary libraries, which includes numpy for numerical operations, keras for building deep learning models, and sklearn for generating the classification data and splitting it into training and testing sets. We also set a random seed to ensure that the results are reproducible."
      ],
      "metadata": {
        "id": "8aLjkfGhWiu0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8S-5jw7zTulz"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# set random seed for reproducibility\n",
        "np.random.seed(123)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define some hyperparameters for the model, including the number of input features, the number of output classes, the number of hidden units in each layer, the number of layers in the model, the batch size for training, the number of epochs for training, the learning rate for the optimizer, and the dropout rate for regularization.\n",
        "\n"
      ],
      "metadata": {
        "id": "hb6GLpPEWrsp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "input_dim = 20  # number of input features\n",
        "output_dim = 2  # number of output classes\n",
        "hidden_dim = 64  # number of hidden units in each layer\n",
        "num_layers = 3  # number of layers in the model\n",
        "batch_size = 128  # batch size for training\n",
        "num_epochs = 50  # number of epochs for training\n",
        "learning_rate = 0.001  # learning rate for optimizer\n",
        "dropout_rate = 0.2  # dropout rate for regularization\n"
      ],
      "metadata": {
        "id": "9rv0za56WpHY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We generate some random data for binary classification using the make_classification function from sklearn. This function creates a synthetic dataset with a specified number of samples, input features, output classes, and informative features, as well as a random seed for reproducibility."
      ],
      "metadata": {
        "id": "EUY0SxW6WytB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate some random data for binary classification\n",
        "X, y = make_classification(n_samples=10000, n_features=input_dim,\n",
        "                            n_classes=output_dim, n_informative=10,\n",
        "                            random_state=123)\n"
      ],
      "metadata": {
        "id": "OoqWPyK_W2Dg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We split the data into training and testing sets using train_test_split from sklearn. This function randomly splits the data into training and testing sets, with a specified test size and random seed for reproducibility."
      ],
      "metadata": {
        "id": "HyvHBHpeW42h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    random_state=123)\n"
      ],
      "metadata": {
        "id": "JYykvleHW77n"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a new model using the Sequential class from keras. This class allows us to stack multiple layers of neurons sequentially to create a deep learning model. We then add the input layer, which is a Dense layer with a specified number of hidden units, input dimensions, and activation function. We also add a Dropout layer after the input layer to randomly drop out a fraction of the neurons during training to reduce overfitting. We then add multiple hidden layers in a for loop, with each layer being a Dense layer with a specified number of hidden units and activation function, followed by a Dropout layer to regularize the model. Finally, we add the output layer, which is a `D"
      ],
      "metadata": {
        "id": "QepFJkUlW-MZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a new model\n",
        "model = Sequential()\n",
        "\n",
        "# add input layer\n",
        "model.add(Dense(hidden_dim, input_dim=input_dim, activation='relu'))\n",
        "model.add(Dropout(dropout_rate))\n",
        "\n",
        "# add hidden layers\n",
        "for i in range(num_layers-1):\n",
        "    model.add(Dense(hidden_dim, activation='relu'))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "# add output layer\n",
        "model.add(Dense(output_dim, activation='softmax'))\n"
      ],
      "metadata": {
        "id": "xR4KEWK9XBtU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We compile the model by specifying the optimizer, loss function, and evaluation metric. Here we use the Adam optimizer with a specified learning rate, the categorical cross-entropy loss function for multi-class classification problems, and accuracy as the evaluation metric."
      ],
      "metadata": {
        "id": "5xsJEbljYIYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=Adam(lr=learning_rate),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uZ-2owaCXk1P"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We train the model using the fit method on the model object. We pass in the training data and labels, the batch size for training, the number of epochs to train for, and the validation data and labels to use for evaluating the model's performance on a separate dataset during training."
      ],
      "metadata": {
        "id": "n5QiPsnCYJz8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=num_epochs,\n",
        "          validation_data=(X_test, y_test),\n",
        "          verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlOj1cVVYLuJ",
        "outputId": "12ba3859-851c-425a-d9b0-232f5811cbd7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "63/63 [==============================] - 1s 8ms/step - loss: 0.5000 - accuracy: 0.7516 - val_loss: 0.3278 - val_accuracy: 0.8645\n",
            "Epoch 2/50\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.3619 - accuracy: 0.8366 - val_loss: 0.2524 - val_accuracy: 0.9050\n",
            "Epoch 3/50\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.2988 - accuracy: 0.8800 - val_loss: 0.2087 - val_accuracy: 0.9220\n",
            "Epoch 4/50\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.2697 - accuracy: 0.8916 - val_loss: 0.1886 - val_accuracy: 0.9280\n",
            "Epoch 5/50\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.2375 - accuracy: 0.9071 - val_loss: 0.1803 - val_accuracy: 0.9335\n",
            "Epoch 6/50\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.2180 - accuracy: 0.9151 - val_loss: 0.1676 - val_accuracy: 0.9390\n",
            "Epoch 7/50\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.2071 - accuracy: 0.9202 - val_loss: 0.1615 - val_accuracy: 0.9385\n",
            "Epoch 8/50\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.2027 - accuracy: 0.9256 - val_loss: 0.1565 - val_accuracy: 0.9410\n",
            "Epoch 9/50\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.1935 - accuracy: 0.9276 - val_loss: 0.1534 - val_accuracy: 0.9400\n",
            "Epoch 10/50\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.1811 - accuracy: 0.9381 - val_loss: 0.1505 - val_accuracy: 0.9455\n",
            "Epoch 11/50\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.1855 - accuracy: 0.9309 - val_loss: 0.1509 - val_accuracy: 0.9415\n",
            "Epoch 12/50\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.1713 - accuracy: 0.9396 - val_loss: 0.1455 - val_accuracy: 0.9460\n",
            "Epoch 13/50\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.1657 - accuracy: 0.9430 - val_loss: 0.1432 - val_accuracy: 0.9480\n",
            "Epoch 14/50\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.1618 - accuracy: 0.9438 - val_loss: 0.1416 - val_accuracy: 0.9500\n",
            "Epoch 15/50\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.1590 - accuracy: 0.9465 - val_loss: 0.1401 - val_accuracy: 0.9470\n",
            "Epoch 16/50\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.1561 - accuracy: 0.9471 - val_loss: 0.1378 - val_accuracy: 0.9515\n",
            "Epoch 17/50\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.1540 - accuracy: 0.9476 - val_loss: 0.1355 - val_accuracy: 0.9510\n",
            "Epoch 18/50\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.1402 - accuracy: 0.9520 - val_loss: 0.1325 - val_accuracy: 0.9555\n",
            "Epoch 19/50\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.1447 - accuracy: 0.9506 - val_loss: 0.1309 - val_accuracy: 0.9580\n",
            "Epoch 20/50\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.1433 - accuracy: 0.9516 - val_loss: 0.1295 - val_accuracy: 0.9560\n",
            "Epoch 21/50\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.1362 - accuracy: 0.9569 - val_loss: 0.1249 - val_accuracy: 0.9600\n",
            "Epoch 22/50\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.1316 - accuracy: 0.9561 - val_loss: 0.1264 - val_accuracy: 0.9580\n",
            "Epoch 23/50\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.1298 - accuracy: 0.9579 - val_loss: 0.1224 - val_accuracy: 0.9590\n",
            "Epoch 24/50\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.1223 - accuracy: 0.9588 - val_loss: 0.1235 - val_accuracy: 0.9615\n",
            "Epoch 25/50\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.1188 - accuracy: 0.9595 - val_loss: 0.1203 - val_accuracy: 0.9620\n",
            "Epoch 26/50\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.1178 - accuracy: 0.9617 - val_loss: 0.1190 - val_accuracy: 0.9635\n",
            "Epoch 27/50\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.1179 - accuracy: 0.9616 - val_loss: 0.1133 - val_accuracy: 0.9650\n",
            "Epoch 28/50\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.1149 - accuracy: 0.9629 - val_loss: 0.1164 - val_accuracy: 0.9620\n",
            "Epoch 29/50\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.1112 - accuracy: 0.9630 - val_loss: 0.1105 - val_accuracy: 0.9670\n",
            "Epoch 30/50\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.1099 - accuracy: 0.9656 - val_loss: 0.1098 - val_accuracy: 0.9660\n",
            "Epoch 31/50\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.1083 - accuracy: 0.9649 - val_loss: 0.1122 - val_accuracy: 0.9675\n",
            "Epoch 32/50\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.1060 - accuracy: 0.9664 - val_loss: 0.1080 - val_accuracy: 0.9665\n",
            "Epoch 33/50\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.1045 - accuracy: 0.9665 - val_loss: 0.1095 - val_accuracy: 0.9655\n",
            "Epoch 34/50\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.1024 - accuracy: 0.9670 - val_loss: 0.1062 - val_accuracy: 0.9665\n",
            "Epoch 35/50\n",
            "63/63 [==============================] - 0s 4ms/step - loss: 0.1037 - accuracy: 0.9685 - val_loss: 0.1041 - val_accuracy: 0.9675\n",
            "Epoch 36/50\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0979 - accuracy: 0.9693 - val_loss: 0.1035 - val_accuracy: 0.9685\n",
            "Epoch 37/50\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0974 - accuracy: 0.9694 - val_loss: 0.1067 - val_accuracy: 0.9695\n",
            "Epoch 38/50\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0925 - accuracy: 0.9695 - val_loss: 0.1045 - val_accuracy: 0.9680\n",
            "Epoch 39/50\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0920 - accuracy: 0.9726 - val_loss: 0.1053 - val_accuracy: 0.9680\n",
            "Epoch 40/50\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0911 - accuracy: 0.9716 - val_loss: 0.1012 - val_accuracy: 0.9715\n",
            "Epoch 41/50\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0914 - accuracy: 0.9737 - val_loss: 0.1040 - val_accuracy: 0.9725\n",
            "Epoch 42/50\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0867 - accuracy: 0.9705 - val_loss: 0.1078 - val_accuracy: 0.9690\n",
            "Epoch 43/50\n",
            "63/63 [==============================] - 0s 5ms/step - loss: 0.0864 - accuracy: 0.9729 - val_loss: 0.1074 - val_accuracy: 0.9685\n",
            "Epoch 44/50\n",
            "63/63 [==============================] - 0s 6ms/step - loss: 0.0832 - accuracy: 0.9740 - val_loss: 0.1017 - val_accuracy: 0.9695\n",
            "Epoch 45/50\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.0828 - accuracy: 0.9755 - val_loss: 0.1046 - val_accuracy: 0.9705\n",
            "Epoch 46/50\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.0835 - accuracy: 0.9739 - val_loss: 0.1032 - val_accuracy: 0.9730\n",
            "Epoch 47/50\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.0825 - accuracy: 0.9747 - val_loss: 0.1017 - val_accuracy: 0.9690\n",
            "Epoch 48/50\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.0800 - accuracy: 0.9759 - val_loss: 0.1024 - val_accuracy: 0.9715\n",
            "Epoch 49/50\n",
            "63/63 [==============================] - 0s 8ms/step - loss: 0.0759 - accuracy: 0.9756 - val_loss: 0.0995 - val_accuracy: 0.9700\n",
            "Epoch 50/50\n",
            "63/63 [==============================] - 0s 7ms/step - loss: 0.0823 - accuracy: 0.9755 - val_loss: 0.1001 - val_accuracy: 0.9720\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9f5e7ce790>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We evaluate the trained model on the test set using the evaluate method on the model object. We pass in the test data and labels, and set verbose=0 to suppress the output of the evaluation process. We then print out the test loss and accuracy of the model."
      ],
      "metadata": {
        "id": "nmxDdx7XYSUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test loss:', loss)\n",
        "print('Test accuracy:', accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caO3opHCYW-f",
        "outputId": "fb0773a2-be69-4349-83ec-63b07e87b180"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.10007339715957642\n",
            "Test accuracy: 0.972000002861023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we generate some new random data and make predictions on it using the predict method on the model object. We pass in the new data to make predictions on, and then print out the new data and the corresponding predictions of the model.\n"
      ],
      "metadata": {
        "id": "m8P84AmNYb-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make predictions on new data\n",
        "new_data = np.random.rand(5, input_dim)\n",
        "predictions = model.predict(new_data)\n",
        "print('New data:', new_data)\n",
        "print('Predictions:', predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18yQGg5KYegE",
        "outputId": "aced50f2-4934-4746-b71d-da3cfdfd12bf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 123ms/step\n",
            "New data: [[0.69646919 0.28613933 0.22685145 0.55131477 0.71946897 0.42310646\n",
            "  0.9807642  0.68482974 0.4809319  0.39211752 0.34317802 0.72904971\n",
            "  0.43857224 0.0596779  0.39804426 0.73799541 0.18249173 0.17545176\n",
            "  0.53155137 0.53182759]\n",
            " [0.63440096 0.84943179 0.72445532 0.61102351 0.72244338 0.32295891\n",
            "  0.36178866 0.22826323 0.29371405 0.63097612 0.09210494 0.43370117\n",
            "  0.43086276 0.4936851  0.42583029 0.31226122 0.42635131 0.89338916\n",
            "  0.94416002 0.50183668]\n",
            " [0.62395295 0.1156184  0.31728548 0.41482621 0.86630916 0.25045537\n",
            "  0.48303426 0.98555979 0.51948512 0.61289453 0.12062867 0.8263408\n",
            "  0.60306013 0.54506801 0.34276383 0.30412079 0.41702221 0.68130077\n",
            "  0.87545684 0.51042234]\n",
            " [0.66931378 0.58593655 0.6249035  0.67468905 0.84234244 0.08319499\n",
            "  0.76368284 0.24366637 0.19422296 0.57245696 0.09571252 0.88532683\n",
            "  0.62724897 0.72341636 0.01612921 0.59443188 0.55678519 0.15895964\n",
            "  0.15307052 0.69552953]\n",
            " [0.31876643 0.6919703  0.55438325 0.38895057 0.92513249 0.84167\n",
            "  0.35739757 0.04359146 0.30476807 0.39818568 0.70495883 0.99535848\n",
            "  0.35591487 0.76254781 0.59317692 0.6917018  0.15112745 0.39887629\n",
            "  0.2408559  0.34345601]]\n",
            "Predictions: [[0.9727245  0.0272755 ]\n",
            " [0.9698933  0.03010667]\n",
            " [0.951587   0.04841296]\n",
            " [0.9608639  0.03913606]\n",
            " [0.9765747  0.0234253 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This entire code creates a deep learning binary classification model with dropout regularization, trains it on a synthetic dataset, evaluates its performance on a test set, and makes predictions on new data. This upgraded version includes the following improvements:\n",
        "\n",
        "Added a Dropout layer after each Dense layer to reduce overfitting.\n",
        "Used the Adam optimizer with a specified learning rate instead of the default optimizer.\n",
        "Used the sparse_categorical_crossentropy loss function for multi-class classification problems.\n",
        "Split the data into training and testing sets using train_test_split from sklearn.\n",
        "Added a validation_data parameter to the fit method to track the validation loss and accuracy during training.\n",
        "Evaluated the model on the test data and printed the test loss and accuracy."
      ],
      "metadata": {
        "id": "zUIjmVnDWFoE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a5DTLVwzWIZI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}